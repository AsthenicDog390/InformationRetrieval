{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Information Retrieval Evaluation Pipeline\n",
    "This notebook provides a template for evaluating query reformulation techniques using PyTerrier.\n",
    "Pipeline stages: Dataset Loading → Preprocessing → Query Reformulation → Retrieval → Evaluation\n",
    "\n"
   ],
   "id": "753a05bdc7331ed0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Dataset Loading\n",
    "This can be switched with another dataset, possibly requiring conversion to this format.\n"
   ],
   "id": "cd5d3d067a6d763"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:51:25.855659Z",
     "start_time": "2025-03-31T09:51:15.213151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "class DatasetComponents:\n",
    "    \"\"\"Container for dataset components that must be provided\"\"\"\n",
    "    def __init__(self, corpus_iter, queries_df, qrels_df):\n",
    "        self.corpus_iter = corpus_iter  # Iterator yielding {'docno': str, 'text': str}\n",
    "        self.queries_df = queries_df    # DataFrame with columns ['qid', 'query']\n",
    "        self.qrels_df = qrels_df        # DataFrame with columns ['qid', 'docno', 'label']\n",
    "\n",
    "def load_pt_dataset():\n",
    "    \"\"\"Load codec dataset\"\"\"\n",
    "    docs = load_dataset(\"macavaney/codec\")[\"train\"]\n",
    "    qrels = load_dataset('irds/codec', 'qrels')\n",
    "    queries = load_dataset('irds/codec', 'queries')\n",
    "\n",
    "    # Convert dataset to correct format\n",
    "    corpus_iter = ({'docno': str(doc['id']), 'text': doc['contents']} for doc in docs)\n",
    "\n",
    "    queries_df = pd.DataFrame(queries)[['query_id', 'query']]\n",
    "    queries_df.columns = ['qid', 'query']\n",
    "\n",
    "    qrels_df = pd.DataFrame(qrels)[['query_id', 'doc_id', 'relevance']]\n",
    "    qrels_df.columns = ['qid', 'docno', 'label']\n",
    "\n",
    "    return DatasetComponents(corpus_iter, queries_df, qrels_df)\n",
    "\n",
    "# Load the dataset\n",
    "data = load_pt_dataset()"
   ],
   "id": "db121cb9c9ddcc18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thein\\anaconda3\\envs\\irllm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Preprocessing Pipeline\n",
    "Currently, this does no preprocessing."
   ],
   "id": "1d6430c8a33bc386"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:54:52.151562Z",
     "start_time": "2025-03-31T09:54:50.647083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not pt.java.started():\n",
    "    pt.java.init()\n",
    "\n",
    "tokeniser = pt.java.autoclass(\"org.terrier.indexing.tokenisation.Tokeniser\").getTokeniser()\n",
    "def strip_markup(text):\n",
    "    return \" \".join(tokeniser.getTokens(text))\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Placeholder for text preprocessing logic\"\"\"\n",
    "    return text\n",
    "\n",
    "def preprocess_corpus(corpus_iter):\n",
    "    \"\"\"Generator that applies preprocessing to each document\"\"\"\n",
    "    for doc in corpus_iter:\n",
    "        yield {\n",
    "            'docno': doc['docno'],\n",
    "            'text': preprocess_text(doc['text'])\n",
    "        }\n",
    "\n",
    "def preprocess_queries(queries_df):\n",
    "    \"\"\"Apply preprocessing to queries dataframe\"\"\"\n",
    "    queries_df = queries_df.copy()\n",
    "    queries_df['query'] = queries_df['query'].apply(strip_markup)\n",
    "    return queries_df\n",
    "\n",
    "# Apply preprocessing while maintaining iterator\n",
    "preprocessed_corpus = preprocess_corpus(data.corpus_iter)\n",
    "preprocessed_queries = preprocess_queries(data.queries_df)"
   ],
   "id": "ad9172b403521b3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Query Reformulation",
   "id": "883dae17c944c37e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:54:56.761654Z",
     "start_time": "2025-03-31T09:54:56.756575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reformulate_queries(queries_df):\n",
    "    \"\"\"Placeholder for query reformulation techniques\"\"\"\n",
    "    modified_queries = queries_df.copy()\n",
    "    # Placeholder for actual reformulation\n",
    "    return modified_queries\n",
    "\n",
    "reformulated_queries = reformulate_queries(preprocessed_queries)"
   ],
   "id": "8cdc0163f153be40",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Indexing Pipeline",
   "id": "f56384466c733ca6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:55:00.767751Z",
     "start_time": "2025-03-31T09:55:00.141765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index_path = Path.cwd() / \"index\"\n",
    "index_ref = None\n",
    "\n",
    "# Check if valid index exists\n",
    "if (index_path / \"data.properties\").exists():\n",
    "    try:\n",
    "        index_ref = pt.IndexFactory.of(str(index_path))\n",
    "        print(f\"Loaded existing index from {index_path}\")\n",
    "\n",
    "        # Verify index contains documents\n",
    "        if index_ref.getCollectionStatistics().getNumberOfDocuments() == 0:\n",
    "            raise ValueError(\"Empty index - will rebuild\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Index loading failed ({str(e)}), rebuilding...\")\n",
    "        import shutil\n",
    "        shutil.rmtree(index_path)\n",
    "        index_ref = None\n",
    "\n",
    "# Build new index if needed\n",
    "if index_ref is None:\n",
    "    print(\"Building new index...\")\n",
    "    index_ref = pt.index.IterDictIndexer(\n",
    "        str(index_path),\n",
    "        meta={\"docno\": 32, \"text\": 131072},\n",
    "        type=pt.index.IndexingType.CLASSIC\n",
    "    ).index(preprocessed_corpus)\n",
    "    print(f\"Built new index at {index_path}\")\n",
    "\n",
    "    print(index_ref.getCollectionStatistics())\n"
   ],
   "id": "14c6f38522030fbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:55:00.694 [main] WARN org.terrier.structures.BaseCompressingMetaIndex -- Structure meta reading data file directly from disk (SLOW) - try index.meta.data-source=fileinmem in the index properties file. 1.4 GiB of memory would be required.\r\n",
      "Loaded existing index from C:\\Users\\thein\\OneDrive\\Documents\\InformationRetrieval\\llm-query-rewriting\\index\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:57:06.425059Z",
     "start_time": "2025-03-31T09:57:06.410989Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ac9d95b751b558b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 729824\n",
      "Number of terms: 941881\n",
      "Number of postings: 159765825\n",
      "Number of fields: 0\n",
      "Number of tokens: 273318564\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:51:27.118136800Z",
     "start_time": "2025-03-28T15:00:10.924155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to delete the index in case it should be recreated\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def delete_index(index_path):\n",
    "    \"\"\"Deletes the index at the specified path.\"\"\"\n",
    "    if index_path.exists():\n",
    "        shutil.rmtree(index_path)\n",
    "        print(f\"Deleted index at {index_path}\")\n",
    "    else:\n",
    "        print(\"No index found to delete.\")"
   ],
   "id": "802bf1529629a611",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted index at C:\\Users\\thein\\OneDrive\\Documents\\InformationRetrieval\\llm-query-rewriting\\index\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Retrieval Setup\n",
    "Currently, using BM25 for retrieval."
   ],
   "id": "6500474760e53a52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T09:57:59.429239Z",
     "start_time": "2025-03-31T09:57:59.387452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bm25 = pt.BatchRetrieve(\n",
    "    index_ref,\n",
    "    wmodel=\"BM25\",\n",
    "    metadata=[\"docno\", \"text\"],\n",
    "    properties={\"termpipelines\": \"\"},\n",
    "    controls={\"qe\": \"off\"}\n",
    ")"
   ],
   "id": "cf327eb8457b516",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thein\\AppData\\Local\\Temp\\ipykernel_23224\\2758154324.py:1: DeprecationWarning: Call to deprecated class BatchRetrieve. (use pt.terrier.Retriever() instead) -- Deprecated since version 0.11.0.\n",
      "  bm25 = pt.BatchRetrieve(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Evaluation Pipeline",
   "id": "9acbe90722ed77a3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-31T10:03:46.793542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run evaluation experiment\n",
    "# metrics: https://pyterrier.readthedocs.io/en/latest/experiments.html#available-evaluation-measures\n",
    "import os\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "# Directory containing JSON files\n",
    "results_dir = \"./results\"\n",
    "\n",
    "# Evaluation metrics\n",
    "eval_metrics = [\"map\", \"ndcg_cut_10\", \"P_10\", \"recall_100\", \"recip_rank\"]\n",
    "\n",
    "results = pt.Experiment(\n",
    "    [bm25],\n",
    "    preprocessed_queries,\n",
    "    data.qrels_df,\n",
    "    eval_metrics,\n",
    "    names=[\"BM25 Baseline\"],\n",
    "    baseline=0\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"Results for no query reformulation:\")\n",
    "display(HTML(results.to_html(index=False)))\n",
    "\n",
    "\n",
    "# Loop through JSON files in the directory\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(results_dir, filename)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data_json = json.load(f)\n",
    "\n",
    "            # Convert JSON data to DataFrame\n",
    "            queries_df = pd.DataFrame(data_json)[[\"query_id\", \"query\"]]\n",
    "            queries_df.columns = [\"qid\", \"query\"]\n",
    "            queries_df['query'] = queries_df['query'].apply(strip_markup)\n",
    "\n",
    "            print(f\"Processing file: {filename}\")\n",
    "\n",
    "            results = pt.Experiment(\n",
    "                [bm25],\n",
    "                queries_df,\n",
    "                data.qrels_df,\n",
    "                eval_metrics,\n",
    "                names=[\"BM25 Baseline\"],\n",
    "                baseline=0\n",
    "            )\n",
    "\n",
    "            # Display the results\n",
    "            display(HTML(results.to_html(index=False)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n"
   ],
   "id": "d164eee1c656fe26",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thein\\AppData\\Local\\Temp\\ipykernel_23224\\1737640262.py:5: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
